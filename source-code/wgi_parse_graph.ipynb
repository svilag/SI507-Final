{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URLS = \"./urls.json\"\n",
    "CACHE = {}\n",
    "CACHE_PATH = './cache/cache.json'\n",
    "\n",
    "# Classes\n",
    "\n",
    "@dataclass()\n",
    "class Group:\n",
    "    \"\"\"creates a group class object\"\"\"\n",
    "    name: str\n",
    "    class_level: str\n",
    "    location: str\n",
    "    competitions: dict\n",
    "\n",
    "    def jsonify(self):\n",
    "        \"\"\"returns class object as json\"\"\"\n",
    "        group_json = {\n",
    "            \"name\": self.name,\n",
    "            \"class_level\": self.class_level,\n",
    "            \"location\": self.location,\n",
    "            \"competitions\": self.competitions\n",
    "        }\n",
    "        return group_json\n",
    "\n",
    "@dataclass()\n",
    "class Competition:\n",
    "    \"\"\"creates a competition class object\"\"\"\n",
    "    title: str\n",
    "    date: str\n",
    "    scores: str\n",
    "    recap: str\n",
    "    groups: list\n",
    "    scores_by_group: dict\n",
    "\n",
    "    def jsonify(self):\n",
    "        \"\"\"returns class object as json\"\"\"\n",
    "        comp_json = {\n",
    "            \"title\": self.title,\n",
    "            \"date\": self.date,\n",
    "            \"scores\": self.scores,\n",
    "            \"recap\": self.recap,\n",
    "            \"groups\": [group.name for group in self.groups],\n",
    "            \"scores_by_group\": self.scores_by_group\n",
    "        }\n",
    "        return comp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filepath:str, encoding='utf-8') -> dict:\n",
    "    \"\"\"Reads a json file and returns a dictionary of the object\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding=encoding) as file_obj:\n",
    "        return json.load(file_obj)\n",
    "\n",
    "def write_json(filepath:str, data, encoding='utf-8', ensure_ascii=False, indent=4):\n",
    "    \"\"\" Serializes object as JSON. Writes content to the provided filepath.\n",
    "        Appends to the end of the file. Checks if filepath exists.\n",
    "        If not, appends file creating a new one if the file does not exists,\n",
    "        else writes over the file. If add=True, appends to the end of the file.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): the path to the file\n",
    "        data (dict)/(list): the data to be encoded as JSON and written to the file\n",
    "        encoding (str): name of encoding used to encode the file\n",
    "        ensure_ascii (str): if False non-ASCII characters are printed as is; otherwise\n",
    "                            non-ASCII characters are escaped.\n",
    "        indent (int): number of \"pretty printed\" indention spaces applied to encoded JSON\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        with open(filepath, 'a', encoding=encoding) as file_obj:\n",
    "            json.dump(data, file_obj, ensure_ascii=ensure_ascii, indent=indent)\n",
    "    else:\n",
    "        with open(filepath, 'w', encoding=encoding) as file_obj:\n",
    "            json.dump(data, file_obj, ensure_ascii=ensure_ascii, indent=indent)\n",
    "\n",
    "def get_content(url:str) -> str:\n",
    "    \"\"\"Takes a url and returns the html content\n",
    "\n",
    "    params:\n",
    "        url(string): link to html content\n",
    "\n",
    "    returns:\n",
    "        response.text: html content\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    # logger.info(\"Fetching content from %s...\", url)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def read_cache() -> dict:\n",
    "    \"\"\" reads the cache file\n",
    "        returns the cache\n",
    "    \"\"\"\n",
    "    cache = read_json(CACHE_PATH)\n",
    "    # logger.info(\"Reading from cache...\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "def check_cache(url:str) -> str:\n",
    "    \"\"\" checks cache for url.\n",
    "        if url not in cache, adds it and rewrites the cache.\n",
    "        returns html content\n",
    "    \"\"\"\n",
    "    cache = read_cache()\n",
    "    if url not in cache.keys():\n",
    "        cache[url] = get_content(url)\n",
    "        write_json(CACHE_PATH, cache)\n",
    "    #     logger.info(\"Writing %s to cache...\", url)\n",
    "    # logger.info(\"Fetching %s from cache...\", url)\n",
    "    return cache[url]\n",
    "\n",
    "\n",
    "def get_competitions(url):\n",
    "    \"\"\"gets competition data\n",
    "    \"\"\"\n",
    "    html_data = check_cache(url)\n",
    "\n",
    "    soup = bs(html_data, 'html.parser')\n",
    "    # logger.info(\"Parsing page: %s\", url)\n",
    "\n",
    "    table_rows = soup.find_all('tr') # list of table rows\n",
    "\n",
    "    comps = []\n",
    "\n",
    "    for t_r in table_rows[:-1]:\n",
    "        tr_children = [child for child in t_r.children] # find children\n",
    "        if len(tr_children) == 3: # rows with dates\n",
    "            year = re.search(\"20..\", url).group()\n",
    "            date = f\"{tr_children[1:-1:1][0].strong.contents[0]}, {year}\" # month day, year\n",
    "        elif len(tr_children) == 9: # rows with groups and scores\n",
    "            tr_children_data = tr_children[1::2][1:]\n",
    "            comp_name = tr_children_data[0].contents[0] # get competition name # list\n",
    "\n",
    "            # check if there is a link to scores # some don't have scores\n",
    "            if tr_children_data[1].a:\n",
    "                scores = tr_children_data[1].a['href'] # get link to scores\n",
    "            else:\n",
    "                scores = \"No scores\"\n",
    "                # logger.info(\"No scores found for %s\", comp_name)\n",
    "\n",
    "            # check if there is a link to recaps # some don't have recaps\n",
    "            if tr_children_data[-1].a:\n",
    "                recaps = tr_children_data[-1].a['href'] # get link to recaps\n",
    "            else:\n",
    "                recaps = \"No recaps\"\n",
    "                # logger.info(\"No recaps found for %s\", comp_name)\n",
    "\n",
    "            # create Competition obj with placeholder for scores_by_group\n",
    "            comp_data = Competition(comp_name, date, scores, recaps, [], {})\n",
    "            comps.append(comp_data)\n",
    "    return comps\n",
    "\n",
    "def get_groups_scores(comp_obj:Competition) -> tuple:\n",
    "    \"\"\" parses page with scores.\n",
    "        Returns a tuple that contains a list of Groups and a dictionary of scores_by_group\n",
    "    \"\"\"\n",
    "    scores_page = comp_obj.scores # url for scores page\n",
    "    if \"https://\" not in comp_obj.scores : # == \"No scores\"\n",
    "        scores_by_group = {\"No groups participated in this competition\": \"No scores to report\"}\n",
    "    else:\n",
    "        html_data = check_cache(scores_page)\n",
    "\n",
    "        soup = bs(html_data, 'html.parser')\n",
    "        # logger.info(\"Parsing page: %s\", scores_page)\n",
    "\n",
    "        groups_list = [] # list of Group objects\n",
    "        scores_by_group = {}\n",
    "\n",
    "        scores_div = soup.find_all('div', attrs={'class': 'table-responsive'}) # list of divs\n",
    "        scores_table = scores_div[0].table\n",
    "        table_rows = scores_table.find_all('tr')\n",
    "\n",
    "        for row in table_rows:\n",
    "            tcells = list(row.children)\n",
    "            if len(tcells) == 3: # rows with class levels\n",
    "                class_level = tcells[1].b.contents[0] # group class_level\n",
    "            elif len(tcells) == 4: # rows with group names\n",
    "                em = tcells[2].contents[1]\n",
    "                group_name = tcells[2].contents[0].strip() # group names\n",
    "                location = em.contents[0].replace('(', '').replace(')', '') # group location\n",
    "\n",
    "                score = tcells[-1].b.contents[0]\n",
    "\n",
    "                group_data = Group(group_name, class_level, location, [])\n",
    "                groups_list.append(group_data)\n",
    "\n",
    "                scores_by_group[group_data.name] = score\n",
    "\n",
    "        return groups_list, scores_by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(\n",
    "#     format='%(levelname)s: %(message)s',\n",
    "#     level=logging.DEBUG\n",
    "# )\n",
    "\n",
    "# Create logger\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# Create logger filename and path\n",
    "LOGPATH = \"./wgi_score_parser_log.log\"\n",
    "\n",
    "# Add logger file and stream handlers\n",
    "# logger.addHandler(logging.FileHandler(LOGPATH)) # write log to file\n",
    "# logger.addHandler(logging.StreamHandler(sys.stdout)) # stream log to stdout\n",
    "\n",
    "# Start logger\n",
    "start_date_time = dt.datetime.now()\n",
    "# logger.info(f\"Start run: {start_date_time.isoformat()}\")\n",
    "# logger.info(\"Start run: %s\", start_date_time.isoformat()) # log start time\n",
    "\n",
    "\n",
    "all_competitions = [] # store competition objects\n",
    "all_groups = [] # store group objects # before cleaning\n",
    "\n",
    "comps_to_write = [] # competitions to write\n",
    "groups_to_write = [] # groups to write # need to clean\n",
    "\n",
    "\n",
    "for link in read_json(URLS):\n",
    "    # check_cache(link)\n",
    "    # get_competitions\n",
    "    competitions = get_competitions(link)\n",
    "\n",
    "    # get groups and score data for each competition\n",
    "    for comp in competitions:\n",
    "        groups_and_scores = get_groups_scores(comp)\n",
    "\n",
    "        if isinstance(groups_and_scores, tuple):\n",
    "            groups, scores = groups_and_scores\n",
    "            comp.groups = groups # update Competition class obj with groups list\n",
    "            comp.scores_by_group = scores # update Competition class obj with scores dict\n",
    "\n",
    "            if len(groups) > 1:\n",
    "                for group in groups:\n",
    "                    all_groups.append(group)\n",
    "\n",
    "        all_competitions.append(comp)\n",
    "\n",
    "        comps_to_write.append(comp.jsonify())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for group in all_groups:\n",
    "    # dict of comps and scores\n",
    "    group_comps = {}\n",
    "    for comp in all_competitions:\n",
    "        if group in comp.groups:\n",
    "            group_comps[f\"{comp.title}, {comp.date}\"] = comp.scores_by_group.get(group.name) # get score for that group\n",
    "    group.competitions = group_comps\n",
    "\n",
    "    groups_to_write.append(group.jsonify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_clean = sorted(groups_to_write, key=lambda x: (x['name'], x['class_level']))\n",
    "\n",
    "clean_groups = [] # json\n",
    "done_cleaning = [] # json\n",
    "\n",
    "# find duplicate group entries, combine competitions, normalize locations\n",
    "for i, group in enumerate(groups_to_clean):\n",
    "    if len(clean_groups) == 0:\n",
    "        clean_groups.append(group)\n",
    "    elif group['name'] == clean_groups[-1]['name'] and group['class_level'] == clean_groups[-1]['class_level']:\n",
    "        if clean_groups[-1]['competitions'] != group['competitions']:\n",
    "            clean_groups[-1]['competitions'] |= group['competitions']\n",
    "            if clean_groups[-1] not in done_cleaning:\n",
    "                done_cleaning.append(clean_groups[-1])\n",
    "    else:\n",
    "        clean_groups.append(group)\n",
    "        if clean_groups[-1] not in done_cleaning:\n",
    "            done_cleaning.append(clean_groups[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex():\n",
    "    def __init__(self, key):\n",
    "        self.id = key\n",
    "        self.connected_to = {}\n",
    "\n",
    "    def add_neighbor(self, nbr, weight=0):\n",
    "        self.connected_to[nbr] = weight\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, nbr):\n",
    "        return self.connected_to[nbr]\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.connected_to.keys()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{str(self.id)} is connected to {str([x.id for x in self.connected_to])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self):\n",
    "        self.vert_list = {}\n",
    "        self.num_verts = 0\n",
    "\n",
    "    def add_vertex(self, key):\n",
    "        self.num_verts += 1\n",
    "        new_vert = Vertex(key)\n",
    "        self.vert_list[key] = new_vert\n",
    "        return new_vert\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_list:\n",
    "            return self.vert_list[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __contains__(self, n):\n",
    "        return n in self.vert_list\n",
    "\n",
    "    def add_edge (self, f, t, weight=0):\n",
    "        if f not in self.vert_list:\n",
    "            nv = self.add_vertex(f)\n",
    "        if t not in self.vert_list:\n",
    "            nv = self.add_vertex(t)\n",
    "        self.vert_list[f].add_neighbor(self.vert_list[t], weight)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_list.keys()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "groups = done_cleaning # Group objects after cleaning\n",
    "competitions = all_competitions # Competition objects\n",
    "\n",
    "# print(len(groups))\n",
    "# print(len(competitions))\n",
    "\n",
    "wgi_graph = Graph()\n",
    "\n",
    "for comp in competitions:\n",
    "    for group in comp.groups:\n",
    "        wgi_graph.add_edge(f\"{comp.title}, {comp.date}\", group.name, comp.scores_by_group[group.name])\n",
    "        wgi_graph.add_edge(group.name, f\"{comp.title}, {comp.date}\", comp.scores_by_group[group.name])\n",
    "\n",
    "# print([[vert.id for vert in vert.connected_to] for vert in wgi_graph.vert_list.values()])\n",
    "\n",
    "# for id, vert in wgi_graph.vert_list.items():\n",
    "#     print((id, [vert.id for vert in vert.connected_to]))\n",
    "\n",
    "# TODO how to write graph to json???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "n = wgi_graph.num_verts\n",
    "\n",
    "discovered = [False] * n\n",
    "\n",
    "def iterativeDFS(graph, v, discovered):\n",
    "\n",
    "    # create a stack used to do iterative DFS\n",
    "    stack = deque()\n",
    "\n",
    "    # push the source node into the stack\n",
    "    stack.append(v)\n",
    "\n",
    "    # loop till stack is empty\n",
    "    while stack:\n",
    "\n",
    "        # Pop a vertex from the stack\n",
    "        v = stack.pop()\n",
    "\n",
    "        # if the vertex is already discovered yet, ignore it\n",
    "        if discovered[v]:\n",
    "            continue\n",
    "\n",
    "        # we will reach here if the popped vertex `v` is not discovered yet;\n",
    "        # print `v` and process its undiscovered adjacent nodes into the stack\n",
    "        discovered[v] = True\n",
    "        print(v, end=' ')\n",
    "\n",
    "        # do for every edge (v, u)\n",
    "        adjList = graph.adjList[v]\n",
    "        for i in reversed(range(len(adjList))):\n",
    "            u = adjList[i]\n",
    "            if not discovered[u]:\n",
    "                stack.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 'PIO Prelims, April 21, 2022'\n",
    "# test = iterativeDFS(wgi_graph, x, discovered)\n",
    "\n",
    "# TODO how to implement search???"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb090b1801519df74d461290f6a3ad7983a75dcf94711c681fabb2ead4c463ab"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
